{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've grown a bit accustomed to basic operations in python we're going to jump ahead to how we can load in data using Python. When we start dealing with packages you'll notice (especially with <code>pandas</code>) that *programming begins to revolve around the features provided by the package* rather than the basic python introduced to you in the first section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pandas Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll start off with the simplest of tasks, suppose that I'd like to load in a CSV and get an idea of what the data might look like. Using <code>pandas</code> this is as easy as a one-liner. We can simply load in the data as a table and explore it effortlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas by default uses a <code>,</code> delimiter, although I can specify it to be anything I want it to be. For example a tab is encoded as <code>\\t</code>, a space is encoded as <code>' '</code>, and so on.\n",
    "\n",
    "\n",
    "demo_df is a **DataFrame** which is a special pandas data-type which contains lots of useful functions. From now on when we say <code>DataFrame.somethinghere</code>, what we actually mean is that you type out (depending on which **DataFrame** we're talking about):\n",
    "\n",
    "```python\n",
    "demo_df.somethinghere\n",
    "```\n",
    "\n",
    "That means that <code>somethinghere</code> is common to *all **DataFrames**, not just* <code>demo_df</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the first $n$ rows of our data using the <code>head</code> property of the data that we loaded in using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore we could get a quick summary of what all the columns are as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can summarize our entire data-table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>DataFrame.info</code> gives us a bunch of columns that we can work with in our dataset. For example suppose I just wanted the age data from our table. I can select the 'Age' column in one of two ways:\n",
    "\n",
    "1. demo_df.subject_id\n",
    "2. demo_df['subject_id'] \n",
    "\n",
    "***\n",
    "*Note that this does not work when the columns have spaces! We'll deal with this in a bit*\n",
    "***\n",
    "\n",
    "The second method has the advantage in that you can input a *list* of columns to pull out. For example:\n",
    "```python\n",
    "columns = ['subject_id','Is person male or female?','Education score']\n",
    "demo_df[columns] \n",
    "```\n",
    "or\n",
    "\n",
    "```python\n",
    "demo_df[ ['subject_id','Is person male or female?','Education score'] ]\n",
    "```\n",
    "\n",
    "Both of which will produce equivalent results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional data selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to select rows in which the individual's gender is female? This is called conditional selection and is incredibly useful for exploring, cleaning and analyzing data! Conditional selection is typically done through a two step process:\n",
    "\n",
    "- Step 1: Pick a condition to mask with\n",
    "- Step 2: Pull out rows/columns that meet this condition\n",
    "\n",
    "We can do this in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>female_mask</code> is a *conditional series*, meaning that for each **index** (the left-hand number) there is an associated value (True or False). To select rows from the original DataFrame <code>demo_df</code> in which <code>female_mask</code> is <code>True</code>, we use the <code>df.loc</code> function, which works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>df.loc</code> also allows use to set columns, the general syntax is:\n",
    "\n",
    "```python\n",
    "df.loc[rows,columns]\n",
    "```\n",
    "So if we wanted to get the subject ids in which the individual is a female, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how every entry in \"Is person male or female?\" is female. Using a mask we've selected only the rows in which the indivdual is a female. We can confirm that we have selected a subset of the dataframe by looking at the **index** (left-hand side), in which some numbers are skipped meaning that some rows (males) have been skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "NOTE: An alternative way to apply a mask instead of <code>demo_df.loc</code> is as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "demo_df[female_mask]\n",
    "```\n",
    "\n",
    "The issue with this is that it isn't *explicit whether we are subsetting our data by rows or columns*. For example if we had a column called <code>True</code>, then it might give us weird behaviours! Using <code>df.loc</code> ensures that we *explicitly tell pandas which rows and columns we're choosing*\n",
    "\n",
    "See https://stackoverflow.com/questions/38886080/python-pandas-series-why-use-loc for more info\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in Multiple Data Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the data that we're working with a spread out across multiple tables. Ideally we'd like to have all this data available in one table for us to work with. Let's load in all the data available to us and take a look at what we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like these dataframes store different types of data! Our final exploration of the data will be reliant on being able to integrate across these differnet measurements. We can perform merging of this data through a process called *merging* or *joining* which is accomplished through <code>pd.merge</code>. Table merging relies on the ability to link separate data tables through a common **key**. \n",
    "\n",
    "The **key** is described as a column of data which uniquely identifies each row. In our case <code>subject_id</code> is a unique key across each of our data tables. Therefore we'd like to *merge our data on the <code>subject_id</code> key*. There are two ways to accomplish this:\n",
    "\n",
    "1. <code>pd.merge(df1,df2,...)</code> \n",
    "2. <code>df1.merge(df2,...)</code>\n",
    "\n",
    "\n",
    "Both methods are equivalent and method 2 saves a bit of typing if you're the lazy kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the data that we acquire isn't in a format that makes sense to directly merge into our DataFrame. The data may be messy, or in a format that needs to be pre-processed in some way in order to be useful to us. Take for example the reaction time dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all notice the following components:\n",
    "1. Each column maps onto a subject on our main DataFrame\n",
    "2. Each row is a recording from a trial, there are 250 entries\n",
    "\n",
    "The raw reaction times from each trial is noisy and not entirely useful for us. We'd like to take the average of each column. That is, we'd like to *apply* a function which takes the average of each column! Doing this is straightforward with pandas using the <code>DataFrame.apply</code> function! This function generally looks like this:\n",
    "\n",
    "\n",
    "```python\n",
    "rt_df.apply(func)\n",
    "```\n",
    "\n",
    "What this does is apply any function <code>func</code> onto each *column of the dataframe rt_df*. If we wanted to apply a function to each *row instead* we'd write it as follows:\n",
    "\n",
    "```python\n",
    "rt_df.apply(func,axis=1)\n",
    "```\n",
    "\n",
    "This will apply a function to each column instead of row! Let's see how this works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each subject ID has their mean reaction time computed! An alternative way to do this (for means specifically, this won't work for other custom functions) is to use <code>DataFrame.mean()</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an equivalent result. Under the hood it's doing the same thing as using <code>DataFrame.apply</code>. Now we need to merge in our data, one might think to use <code>DataFrame.merge</code>, but there is a caveat!\n",
    "\n",
    "\n",
    "mean_rt is no longer a DataFrame! In fact, it is called a **Series**, which is a list of (index,value) pairings. Remember <code>female_mask</code>? *That was also a **Series** object!*. \n",
    "\n",
    "The left-hand side is the index, the right hand-side is the value. So when merging this series into our main DataFrame we need to take this subtlety into consideration! We can get around this by first converting our **Series** into a DataFrame then merging it in as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the **index** on the left-hand side is now the subject keys? When we merge DataFrames we want to make sure we specify this since we don't have a column called 'subject_id' as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We'll deal with the ugly $0$ column name in a bit :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Data cleaning is a multi-step process that typically consists of the following steps:\n",
    "\n",
    "1. Renaming columns so that they're easier to work with\n",
    "2. Dealing NaN values or \"Impossible values\" (for example gender = -20) \n",
    "4. Removing columns that you're not interested in analyzing\n",
    "\n",
    "The process of data cleaning may be extensive and in many cases *is the most extensive part* of analyzing data. In this tutorial we'll quickly go over how one might perform each of the above steps using pandas and end off with a clean dataset that we can start exploring and visualizing.\n",
    "\n",
    "One key point is that data cleaning is sometimes easier if you visualize your data! This is a great thing to do when looking at whether you're dealing with impossible values that should not exist in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the data that we acquire doesn't come in a format that make sense to merge into our dataframe. Either the data is too messy, is in a raw format that isn't directly useful for visualization and analysis or "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several reasons why one might want to rename their columns in an analysis. Here are a few:\n",
    "\n",
    "\n",
    "**Some columns just don't make any sense!** Take a look at one of the columns we're dealing with in our dataset called *Unnamed: 30*. Now tell me what exactly this column is supposed to tell you without looking through the dataset. *You can't*, instead you'll have to do the hard-work of deciphering this or writing it down in your notebook so you can keep track of what columns mean. In this case the solution is to first decipher the column, and there are several ways we could do this, then to *rename the column to one that makes much more sense and aligns with what the data indicates*. \n",
    "\n",
    "\n",
    "Here's another one: Our dataset has a column called 'How many days were you hospitalized for your mental illness'. You could pull this column from your dataset in the following way:\n",
    "\n",
    "```python\n",
    "df['Is person male or female?']\n",
    "```\n",
    "\n",
    "But typiing this out repeatedly is a pain and gets tiring quick. Life would be SO MUCH EASIER if we could just rename this column to access our data like this\n",
    "\n",
    "```python\n",
    "df['gender']\n",
    "```\n",
    "\n",
    "This is succint and gets to the point about what the variable means in the context of this data.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tackle the first problem. First let's grab a list of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Impossible Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data entry is a task that is incredibly prone to error. Sometimes data is collected incorrectly resulting in values that should not exist or are useful for helping us understand the phenomena of interest. First let's lay out some rules associated with our data that we *should know a priori*. \n",
    "\n",
    "- <code>subject_id</code> should be a random alphanumeric string\n",
    "- <code>se_score</code> is a value <100\n",
    "- <code>edu_score</code> is a value <100\n",
    "- <code>gender</code> male, female, or other\n",
    "- <code>group</code> control or case\n",
    "- <code>brain_vol</code> some floating point number that is positive\n",
    "- <code>brain_surfarea</code> some floating point number that is positive\n",
    "- <code>brain_gm_thick</code> some floating point number that is positive but not \"substantially big\"\n",
    "- <code>cog_score</code> some floating point number that is positive\n",
    "\n",
    "Let's quickly check over what kinds of values we have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check numeric types:\n",
    "\n",
    "- se_score\n",
    "- edu_score\n",
    "- brain_vol\n",
    "- brain_surfarea\n",
    "- brai_gm_thick\n",
    "- cog_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can do an overall check on our data. Recall that <code>df.info</code> gives us a high level overview of our data. We can expect that all numeric data (listed above) should be of a numeric data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how:\n",
    "\n",
    "- se_score\n",
    "- cog_score\n",
    "\n",
    "Are non-numeric. This is the first indication that there might be something up with our data. Let's inspect this a bit closer by examining what data is not numeric. \n",
    "\n",
    "First we need to know how to determine whether a value is a number or not. We can do this in many ways, but the method with the least hassle is to use python type-conversion. Recall that we can convert a variable from <code>string</code> to <code>float</code> using the python function <code>float()</code>. Also remember that *python will complain with a <code>ValueError</code>* if it cannot convert the <code>string</code> data into <code>float</code>. Turns out that we can actually use this to our advantage using the nifty <code>try/catch</code> functionality. Let's wrap this in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to *apply* this function <code>is_number_numeric</code>, to each element of our dataframe for our two suspect columns. It turns out that to *apply* a function to each *element* it's as easy as using the function <code>df.applymap</code>. <code>df.applymap(func)</code>, applies the function <code>func</code> to each element of a dataframe <code>df</code> and returns a new <code>DataFrame</code> with each element having the function applied to. This is easier to see in practice, let's use <code>applymap</code> to determine whether each number is numeric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use **conditional masking** in order to find rows in which the element cannot be converted into a <code>float</code>! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a one-liner to achieve the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the values in <code>se_score</code> and <code>cog_score</code> that cannot be converted are 'missing' and 'refused' respectively. These values are effectively useless for our analysis, so we're going to have to replace them with **something**. Now there are two options here when trying to fill in missing or nonsense values:\n",
    "\n",
    "\n",
    "1. Replace them with a value which signals that we should ignore it and proceed on as usual\n",
    "2. Estimate what the value could be based on other information available to us\n",
    "\n",
    "This is the problem of **imputation** which is an active area of research in statistical modelling. We won't dive too deep into this as it is a course all on it's own. For now we'll replace the data with a \"nan\" value which is a special type of value provided for by numpy. The \"nan\" value indicates that this value is meaningless and should be ignored. \n",
    "\n",
    "To do this we can use masking to select and replace rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to fill in these missing values is to use the function <code>pd.to_numeric</code>. This will in effect convert all the data in the specified columns into a numeric type and replace an non-numeric values with a NaN. Why didn't we just use this from the get-go? Because it's important to know *what data you're replacing with NaN first before doing so!* Let's convert se_score and cog_score into numeric types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem fixed! Now let's fix the rest of the dataset is correct using conditional statements as we've done above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check that se_score, edu_score, and brain_gm_thick are not large. Let's say anything greater than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the fix as we've done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you've probably gotten the idea. *We need to extensively check our data before using it for errors!*. I'll just say (since I made up this data), that there are no more issues to resolve. But in reality you would have to perform checks on *all the data you plan to use!*. In any type of analysis workflow, data cleaning is the longest, most boring, but most important component of the workflow! The decisions you make here will influence the rest of your analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset is ready to go! Let's visualize some things to learn interesting aspects of our data! Before we do that we need to save our data, this is an easy one-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUT WAIT**: How do we automate this? Let's say we collect more data down the line and we want to spit out a cleaned up dataset. The answer to this is to download the data as a **Python script**. We won't get into automation in this workshop but it's something that is incredibly easy to do once you've gotten a Jupyter notebook prototype like this one set up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
